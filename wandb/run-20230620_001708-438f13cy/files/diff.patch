diff --git a/__pycache__/config.cpython-310.pyc b/__pycache__/config.cpython-310.pyc
index 25ead20..bc76505 100644
Binary files a/__pycache__/config.cpython-310.pyc and b/__pycache__/config.cpython-310.pyc differ
diff --git a/__pycache__/dataset.cpython-310.pyc b/__pycache__/dataset.cpython-310.pyc
deleted file mode 100644
index fa4b3b3..0000000
Binary files a/__pycache__/dataset.cpython-310.pyc and /dev/null differ
diff --git a/config.py b/config.py
index b8ee66c..029cb77 100644
--- a/config.py
+++ b/config.py
@@ -1,6 +1,15 @@
 from easydict import EasyDict
+
+
 import torch
 CFG = EasyDict()
-CFG.batch = 128
+CFG.batch_size = 128
 CFG.linear = [28*28,14*14,14*14,14*14,10]
-CFG.device = "cuda" if torch.cuda.is_available() else "cpu"
\ No newline at end of file
+CFG.device = "cuda" if torch.cuda.is_available() else "cpu"
+CFG.epochs = 150
+CFG.optim = "torch.optim.SGD"
+CFG.optim_config = {"lr":1e-3, "momentum":0.9}
+CFG.lossfn = torch.nn.CrossEntropyLoss()
+CFG.project = "python-CourseDesign"
+CFG.wandb = True
+CFG.__optim_function = lambda parameter: eval(CFG.optim)(parameter,**CFG.optim_config)
diff --git a/main.py b/main.py
index e79272c..bdb50a1 100644
--- a/main.py
+++ b/main.py
@@ -1,30 +1,28 @@
-import numpy as np
 import torch
 from torch.utils.data import DataLoader
 from torchvision import transforms
-import os
 import torchvision
-import gzip
-import cv2
 import matplotlib.pyplot as plt
-from dataset import DealDataset
+from utils import MNIST,wandb_init
 from config import CFG
 from net import MLP
+from tqdm import tqdm,trange
+import numpy as np
 
-trainDataset = DealDataset('dataset/MNIST/raw', "train-images-idx3-ubyte.gz",
+trainDataset = MNIST('dataset/MNIST/raw', "train-images-idx3-ubyte.gz",
                            "train-labels-idx1-ubyte.gz", transform=transforms.ToTensor())
-testDataset = DealDataset('dataset/MNIST/raw', "t10k-images-idx3-ubyte.gz",
+testDataset = MNIST('dataset/MNIST/raw', "t10k-images-idx3-ubyte.gz",
                           "t10k-labels-idx1-ubyte.gz", transform=transforms.ToTensor())
 
 # 训练数据和测试数据的装载
 train_loader = torch.utils.data.DataLoader(
     dataset=trainDataset,
-    batch_size=CFG.batch,
+    batch_size=CFG.batch_size,
     shuffle=True,
 )
 test_loader = torch.utils.data.DataLoader(
     dataset=testDataset,
-    batch_size=CFG.batch,
+    batch_size=CFG.batch_size,
     shuffle=False,
 )
 
@@ -33,141 +31,72 @@ test_loader = torch.utils.data.DataLoader(
 images, labels = next(iter(train_loader))
 img = torchvision.utils.make_grid(images)
 img = img.numpy().transpose(1, 2, 0)
-a = MLP(CFG.linear).to(CFG.device)
-loss_fn = torch.nn.CrossEntropyLoss()
-optimizer = torch.optim.SGD(a.parameters(), lr=1e-3, momentum=0.9)
 print(labels)
 plt.imshow(img)
 plt.show()
 
+model = MLP(CFG.linear).to(CFG.device)
 
-from torch.autograd import Variable
-from torch.utils.tensorboard import SummaryWriter
-def train(dataloader, model, loss_fn, optimizer):
+loss_fn = CFG.lossfn
+optimizer = CFG.__optim_function(model.parameters())
 
-    # Total size of dataset for reference
-    size = 0
+if CFG.wandb:
+    run = wandb_init(CFG)
+    run.watch(model, log='all')
+    Total_params = 0
+    Trainable_params = 0
+    for param in model.parameters():
+        mulValue = np.prod(param.size())
+        Total_params += mulValue
+        if param.requires_grad:
+            Trainable_params += mulValue
+    run.log({"Total params": Total_params, "Trainable params": Trainable_params})
 
-    # places your model into training mode
+def train(dataloader, model, loss_fn, optimizer):
+    size = 0
     model.train()
-
-    # loss batch
-    batch_loss = {}
-    batch_accuracy = {}
-
-    correct = 0
-    _correct = 0
-
-
-
-    # Gives X , y for each batch
-    for batch, (X, y) in enumerate(dataloader):
-
-        # Converting device to cuda
+    batch_loss = []
+    batch_correct = []
+    for i,(X, y) in enumerate(dataloader):
         X, y = X.to(CFG.device), y.to(CFG.device)
-        # Compute prediction error / loss
-        # 1. Compute y_pred
-        # 2. Compute loss between y and y_pred using selectd loss function
-
         y_pred = model(X.view(-1,28*28))
         loss = loss_fn(y_pred, y)
-
-        # Backpropagation on optimizing for loss
-        # 1. Sets gradients as 0
-        # 2. Compute the gradients using back_prop
-        # 3. update the parameters using the gradients from step 2
-
         optimizer.zero_grad()
         loss.backward()
         optimizer.step()
-
-        _correct = (y_pred.argmax(1) == y).type(torch.float).sum().item()
-        _batch_size = len(X)
-
-        correct += _correct
-
-        # Updating loss_batch and batch_accuracy
-        batch_loss[batch] = loss.item()
-        batch_accuracy[batch] = _correct/_batch_size
-
-        size += _batch_size
-
-        if batch % 100 == 0:
-            loss, current = loss.item(), batch * len(X)
-            print(f"loss: {loss:>7f}  [{current:>5d}]")
-
-    correct/=size
-    print(f"Train Accuracy: {(100*correct):>0.1f}%")
-
-    return batch_loss , batch_accuracy
+        batch_loss.append(loss.item())
+        batch_correct.append((y_pred.argmax(1) == y).sum().item() / len(X))
+        size += len(X)
+    train_loss = sum(batch_loss) / len(batch_loss)
+    train_correct = sum(batch_correct) / len(batch_correct)
+    # tqdm.write(f"Train Error: \t Accuracy: {(100*train_correct):>0.5f}%, Avg loss: {train_loss:>8f} ")
+    return train_correct,train_loss
 
 def validation(dataloader, model, loss_fn):
-
-    # Total size of dataset for reference
     size = 0
-    num_batches = len(dataloader)
-
-    # Setting the model under evaluation mode.
     model.eval()
-
-    test_loss, correct = 0, 0
-
-    _correct = 0
-    _batch_size = 0
-
-    batch_loss = {}
-    batch_accuracy = {}
-
+    batch_loss = []
+    batch_correct = []
     with torch.no_grad():
-
-        # Gives X , y for each batch
-        for batch , (X, y) in enumerate(dataloader):
-
+        for batch, (X, y) in enumerate(dataloader):
             X, y = X.to(CFG.device), y.to(CFG.device)
-            pred = model(X.view(-1,28*28))
-
-            batch_loss[batch] = loss_fn(pred, y).item()
-            test_loss += batch_loss[batch]
-            _batch_size = len(X)
-
-            _correct = (pred.argmax(1) == y).type(torch.float).sum().item()
-            correct += _correct
-
-            size+=_batch_size
-            batch_accuracy[batch] = _correct/_batch_size
-
-
-
-
-    ## Calculating loss based on loss function defined
-    test_loss /= num_batches
-
-    ## Calculating Accuracy based on how many y match with y_pred
-    correct /= size
-
-    print(f"Valid Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
-
-    return batch_loss , batch_accuracy
-
-
-train_batch_loss = []
-train_batch_accuracy = []
-valid_batch_accuracy = []
-valid_batch_loss = []
-train_epoch_no = []
-valid_epoch_no = []
-
-epochs = 100
-for t in range(epochs):
-    print(f"Epoch {t+1}\n-------------------------------")
-    _train_batch_loss , _train_batch_accuracy = train(train_loader, a, loss_fn, optimizer)
-    _valid_batch_loss , _valid_batch_accuracy = validation(test_loader, a, loss_fn)
-    for i in range(len(_train_batch_loss)):
-        train_batch_loss.append(_train_batch_loss[i])
-        train_batch_accuracy.append(_train_batch_accuracy[i])
-        train_epoch_no.append( t + float((i+1)/len(_train_batch_loss)))
-    for i in range(len(_valid_batch_loss)):
-        valid_batch_loss.append(_valid_batch_loss[i])
-        valid_batch_accuracy.append(_valid_batch_accuracy[i])
-        valid_epoch_no.append( t + float((i+1)/len(_valid_batch_loss)))
-print("Done!")
\ No newline at end of file
+            pred = model(X.view(-1, 28*28))
+            batch_loss.append(loss_fn(pred, y).item())
+            batch_correct.append((pred.argmax(1) == y).sum().item() / len(X))
+            size += len(X)
+    test_loss = sum(batch_loss) / len(batch_loss)
+    test_correct = sum(batch_correct) / len(batch_correct)
+    return test_correct,test_loss
+
+
+
+
+with trange(CFG.epochs) as t:
+    for _ in t:
+        train_accuracy,train_loss = train(train_loader, model, loss_fn, optimizer)
+        correct,test_loss = validation(test_loader, model, loss_fn)
+        if CFG.wandb:
+            run.log({"epoch": _, "train_correct": train_accuracy*100, "train_loss": train_loss, "val_correct": correct*100, "test_loss": test_loss})
+        t.set_postfix(train_correct=train_accuracy*100, train_loss=train_loss, val_correct=correct*100, test_loss=test_loss)
+if CFG.wandb:
+    run.finish()
\ No newline at end of file
diff --git a/dataset.py b/utils.py
similarity index 72%
rename from dataset.py
rename to utils.py
index d541915..c7b4fb8 100644
--- a/dataset.py
+++ b/utils.py
@@ -1,8 +1,10 @@
-from torch.utils.data import  Dataset
+from torch.utils.data import Dataset
 import gzip
 import numpy as np
 import os
-class DealDataset(Dataset):
+import wandb
+
+class MNIST(Dataset):
 
     def __init__(self, folder, data_name, label_name, transform=None):
         self.train_set, self.train_labels = self.load_data(folder, data_name, label_name)
@@ -23,4 +25,14 @@ class DealDataset(Dataset):
                 os.path.join(data_folder, data_file), 'rb') as datapath:
             label = np.frombuffer(labelpath.read(), dtype=np.uint8, offset=8)
             data = np.frombuffer(datapath.read(), dtype=np.uint8, offset=16).reshape(len(label), 28, 28)
-        return np.array(data), np.array(label)
\ No newline at end of file
+        return np.array(data), np.array(label)
+
+
+def wandb_init(CFG):
+    run = wandb.init(
+        project=CFG.project,
+        name=f"{CFG.optim}-{CFG.batch_size}",
+        config={k: v for k, v in CFG.items() if '__' not in k},
+        save_code=True
+    )
+    return run
\ No newline at end of file
